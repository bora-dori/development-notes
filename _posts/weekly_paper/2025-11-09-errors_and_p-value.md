---
layout: post
title: "What Really Matters in Hypothesis Testing: Errors and p-values"
date: 2025-11-09 +0900
---
# ðŸŽ¯ Type I & Type II Errors and the Meaning of p-value

Understanding statistical hypothesis testing is essential in data analysis.  
When we test a claim using sample data, we donâ€™t get absolute certainty. We make a **decision based on probability**.  
Thatâ€™s why we must understand **two kinds of possible errors (Type I and Type II)** and the role of the **p-value** in drawing conclusions.

---

## 1. Hypothesis Testing in a Nutshell

Every hypothesis test starts with two opposing statements:
- **Null hypothesis (Hâ‚€)**: The current or default assumption.  
  *Example:* â€œThe new drug has no effect.â€
- **Alternative hypothesis (Hâ‚)**: The claim we want to support.  
  *Example:* â€œThe new drug has an effect.â€

We then collect sample data and calculate a **test statistic** to see how far our data deviate from what would be expected if Hâ‚€ were true.  
Based on this, we compute a **p-value** and compare it to a **significance level (Î±)**, typically set at **0.05** (5%).

---

## 2. Type I and Type II Errors

In hypothesis testing, we never know the â€œtrueâ€ state of the world for certain.  
This means two kinds of mistakes can occur.

| Reality | Decision | Outcome |
|----------|-----------|----------|
| Hâ‚€ is true | Fail to reject Hâ‚€ | âœ… Correct |
| Hâ‚€ is true | Reject Hâ‚€ | âŒ **Type I Error (False Positive)** |
| Hâ‚€ is false | Reject Hâ‚€ | âœ… Correct |
| Hâ‚€ is false | Fail to reject Hâ‚€ | âŒ **Type II Error (False Negative)** |

---

### ðŸ”¹ Type I Error (Î±)

- **Definition:** Rejecting the null hypothesis even though itâ€™s actually true.  
- **Interpretation:** You conclude â€œthere is an effectâ€ when none exists.  
- **Example:** Convicting an innocent person.  
- **Probability of occurrence:** **Î± (alpha)**, the **significance level** set before the test.  
  - If Î± = 0.05 â†’ you accept a 5% chance of wrongly rejecting a true Hâ‚€.

---

### ðŸ”¹ Type II Error (Î²)

- **Definition:** Failing to reject the null hypothesis when itâ€™s actually false.  
- **Interpretation:** You miss a real effect.  
- **Example:** Letting a guilty person go free.  
- **Probability of occurrence:** **Î² (beta)**  
- **Power of the test:** \( 1 - Î² \)  
  - The probability of correctly detecting a true effect when it exists.

---

### âš–ï¸ Trade-off Between Î± and Î²

Thereâ€™s a balance between being **too strict** and **too lenient**:
- If you **lower Î±** (make the test more conservative),  
  â†’ Type I error decreases, but Type II error (Î²) increases.  
- If you **raise Î±**,  
  â†’ Youâ€™re more likely to detect true effects, but also more likely to make false positives.

Thus, most analyses use Î± = 0.05 as a compromise between both risks.

---

## 3. The Role of the Significance Level (Î±)

**Significance level (Î±)** is the threshold that defines how skeptical we are toward new evidence.

> â€œHow strong must the data be before we reject Hâ‚€?â€
- Commonly set to **5% (0.05)**.  
- Meaning: â€œWe accept up to a 5% chance of making a Type I error.â€  
- In critical fields (e.g., medicine, safety), stricter levels like **1% (0.01)** are often used.

---

## 4. What Is the p-value?

The **p-value** quantifies how compatible your data are with the null hypothesis.

> **p-value:**  
> The probability of obtaining results **as extreme as, or more extreme than** the observed data, **assuming the null hypothesis is true.**

### Interpreting the p-value

| Comparison | Statistical Decision | Interpretation |
|-------------|----------------------|----------------|
| **p â‰¤ Î±** | Reject Hâ‚€ | Thereâ€™s statistically significant evidence for Hâ‚. |
| **p > Î±** | Fail to reject Hâ‚€ | Not enough evidence to reject Hâ‚€. |

> Important:  
> A large p-value **does not prove Hâ‚€ true** â€” it only means we lack sufficient evidence against it.

---

## 5. Why p-values Can Be Misleading

The p-value depends heavily on **sample size**.

- With **large samples**, even tiny differences can yield a small p-value (statistically significant but practically meaningless).  
- With **small samples**, meaningful differences may fail to reach significance.

Therefore, **Statistical significance â‰  Practical importance.**  
You must consider both the **effect size** and **context** of the data.

Example:  
In a business A/B test, a 0.1% click-through improvement might be statistically significant (small p-value) but irrelevant if it doesnâ€™t increase profit.

---

## 6. Putting It All Together

1. **Set hypotheses (Hâ‚€, Hâ‚)**  
2. **Decide Î±** (your tolerance for Type I error)  
3. **Collect data & calculate test statistic**  
4. **Compute p-value**  
5. **Compare p-value to Î±**  
6. **Draw conclusions**, keeping in mind the potential for Type I and Type II errors.

---

## Summary

> Type I and Type II errors remind us that no statistical test is perfect.  
> The p-value helps us measure how surprising our results are under the null hypothesis 
> But sound judgment, context, and effect size always matter more than a single number.

| Concept | Definition | Symbol | Interpretation |
|----------|-------------|----------|----------------|
| **Type I Error** | Rejecting a true null hypothesis | Î± | False positive |
| **Type II Error** | Failing to reject a false null hypothesis | Î² | False negative |
| **Test Power** | Correctly detecting a true effect | 1 - Î² | Sensitivity |
| **Significance Level** | Threshold for deciding when to reject Hâ‚€ | Î± | Commonly 0.05 |
| **p-value** | Probability of observing extreme results under Hâ‚€ | â€“ | Smaller â†’ stronger evidence against Hâ‚€ |
